---
title: "Fundamentos de Análisis de Datos"
author: "Noelia Herranz - David Rodríguez"
date: 14/1/2023
output:
  html_document: 
    number_sections: yes
    toc: true
    toc_depth: 4
    toc_float:
      collapsed: true
      smooth_scroll: false
    self_contained: yes   
---
 
Versión:
```{r include=TRUE}
version
```

```{r  include=FALSE, fig.align='center'}
knitr::opts_chunk$set(echo = TRUE)
```

Rúbrica de la asignatura:

*	Uso de herramienta/s de control de versiones (1 punto)
*	Definición de objetivos (1 punto)
*	Análisis exploratorio inicial (1 punto)
*	Detección, tratamiento e imputación de datos faltantes (1 punto)
*	Transformaciones de variables cuantitativas (1 punto)
*	Procesado de variables cualitativas (1 punto)
*	Selección de variables (1 punto)
*	Ajuste, interpretación y diagnosis del modelo de regresión lineal múltiple (2 puntos)
*	Valoración del profesor (1 punto)

  
# Definición de objetivos

Disponemos de una tabla obtenida de Kaggle con información relativa a préstamos personales en EE.UU.. En esta tabla tenemos una variedad de campos tanto numéricos como categoricos que nos aportan a nivel de cliente y para un préstamo personal concreto, información previa a la solicitud del préstamo, edad, ingresos, tiempo trabajado de la persona, tipo de interés, importe y finalidad del préstamo,...y una vez finalizados los plazos de pago, se informa sí ha pagado el préstamo o no.

En este trabajo la finalidad es realizar una regresión múltiple y seleccionaremos como variable dependiente el importe de préstamo, como es obvio es una variable numérica.

Es un ejemplo un poco forzado, dado que, en la realidad, el cliente ya sabe la cantidad que solicita y en algunos casos, si la persona es cliente de la entidad que solicita el préstamo puede que la entidad tenga ya limites preconcedidos de préstamos.


Como decíamos queremos realizar un modelo multivariante para poder estimar el importe del préstamo del cliente con la información disponible previa a la solicitud del préstamo, este matiz es importante, ya que habrá variables que no podamos utilizar porque son variables calculadas a posteriori de la solicitud del préstamo que son calculadas en base al importe del préstamo.

# Librerías



```{r, warning=FALSE, message=FALSE}

#if (!require(nortest)) {install.packages("nortest")}
#if (!require(moments)) {install.packages("moments")}
#if (!require(corrplot)) {install.packages("corrplot")}
#if (!require(VIM)) {install.packages("VIM")}
#if (!require(bestNormalize)) {install.packages("bestNormalize")}
# install.packages(MASS)
#if (!require(leaps)) {install.packages("leaps")}
#if (!require(lmtest)) {install.packages("lmtest")}



library(dplyr)
library(tidyverse)
library(kableExtra)
library(ggplot2)
library(cowplot)
library(corrplot)
library(moments)
library(VIM)
library(nortest)
#library(MASS)
library(bestNormalize)
library(leaps)
#library(GGally)
library(lmtest) 


```

# Análisis exploratorio de datos

Lectura del fichero de datos y de la descripción de las variables:

```{r}
set.seed(1)

setwd("C:/David/Estadística/Master/Analisis de Fundamentos/R/Practica/")


#df <- read.csv('~/máster/Practica_1/credit_risk_dataset.csv', header = TRUE)
df <- read.csv('credit_risk_dataset.csv', header = TRUE)
df <- df %>% dplyr::sample_frac(0.4)
head(df)
dim(df)
```
El dataset consta de 32581 observaciones y 12 variables que se describen a continuación:
```{r}

# Nombre y descripción de variables
#descripcion_variables <- read.csv("~/máster/Practica_1/Descripción de variables.csv", header = TRUE, 
descripcion_variables <- read.csv("Descripcion_variables.csv", header = TRUE, fileEncoding = "Latin1", check.names = F,sep=";")
descripcion_variables %>%
  kbl() %>%
  kable_styling()

```
 
Se observa que las variables clasificación del  préstamo (loan_grade), estado del prestamo (loan_status) y porcentaje del préstamo sobre los ingresos (loan_percent_income), son variables que se calculan en base al importe del préstamo, nuestra variable que queremos predecir por lo tanto no se podrán utilizar como variables independientes. 
 
Codificación de las variables

```{r}
str(df)
```

Se observa que las variables relación con la propiedad (person_home_ownership), motivo del préstamo (loan_intent) y falta de pago de un préstamo antiguo (cb_person_default_on_file) R las considera cadenas, por lo que modificamos el tipo a factor:  


```{r}
df <- df %>%
         mutate(person_home_ownership  = as.factor(person_home_ownership )) %>%
         mutate(loan_intent  = as.factor(loan_intent )) %>%
         mutate(cb_person_default_on_file=as.factor(cb_person_default_on_file))
```


Modificamos el nombre de la variable independiente por target

```{r}
df=df %>% rename(target=loan_amnt)
```


Generamos dos data frame, uno para entrenamiento (train) y otro para validación (validation)

```{r}
#kk
set.seed(1)

#create ID variable
df$id <- 1:nrow(df)

#Use 70% of dataset as training set and remaining 30% as testing set 
train <- df %>% dplyr::sample_frac(0.7)
validacion  <- dplyr::anti_join(df, train, by = 'id')

#Borrar la variable id
train <-subset(train, select= -c(id))
validacion <-subset(validacion, select= -c(id))

```

Tamaño de los dataframe

```{r}
dim(train)
dim(validacion)

```


Antes de comenzar a analizar las variables definimos unas funciones en R para facilitar el código del análisis.

Aunque hay multiples paquetes con resumenes estadísticos, generamos uno adaptado y así "jugamos" un poco con R.
El matiz es que no encontramos ninguno que tenga el porcentaje de ceros por variab.

```{r}

describe_v1 <-function(dataframe,variable) {

  tabla1 <- dataframe %>%  
                                
  summarise(
    cases       = format(n(),big.mark = ","),                                               
    min   = format(min({{variable}}, na.rm = T),big.mark = ","),                    
    p01   = format(quantile({{variable}},c(0.01), na.rm = T ),big.mark = ","),
    p03   = format(quantile({{variable}},c(0.03), na.rm = T ),big.mark = ","),  
    p05   = format(quantile({{variable}},c(0.05), na.rm = T ),big.mark = ","),
    p10   = format(quantile({{variable}},c(0.10), na.rm = T ),big.mark = ","),
    
    mean  = format(round(mean({{variable}}, na.rm=T), digits = 1),big.mark = ","),  
    median  = format(round(median({{variable}}, na.rm=T), digits = 1),big.mark = ","),  
    
    p90   = format(quantile({{variable}},c(0.90), na.rm = T ),big.mark = ","), 
    p95   = format(quantile({{variable}},c(0.95), na.rm = T ),big.mark = ","), 
    p97   = format(quantile({{variable}},c(0.97), na.rm = T ),big.mark = ","),
    p99   = format(quantile({{variable}},c(0.99), na.rm = T ),big.mark = ","),  
    
    max   = format(max({{variable}}, na.rm = T),big.mark = ","),                    
 
    )
 

  return(tabla1)
}

```


```{r} 
describe_v2 <-function(dataframe,variable) {
  
  tabla2 <- dataframe %>%                                        
  summarise(
    sesgo    = format(round(skewness({{variable}}, na.rm = T), digits = 1),big.mark = ","),
    curtosis    = format(round(kurtosis({{variable}}, na.rm = T), digits = 1),big.mark = ","),
    sd    = format(round(sd({{variable}}, na.rm = T), digits = 1),big.mark = ","),  
    missing         = sum(is.na({{variable}}),digits = 1)-1,
    pct_missing = scales::percent(missing / n()) ,
    valor_0     = sum({{variable}} == 0, na.rm = T),               
    pct_valor_0 = scales::percent(valor_0 / n())   
    )  

  return(tabla2)
}

```


```{r}

grafico_numerico2 <-function(dataframe,variable) {
    mean_vble  = round(mean(dataframe$variable, na.rm=T), digits = 1)
    return(mean_vble)
}
#v=grafico_numerico2(df,loan_amnt)
```

```{r}

grafico_numerico <-function(dataframe,variable) {

      #mean_vble  = round(mean(dataframe$variable, na.rm=T), digits = 1)
      
        fill <- "lightblue"
        line <- "darkblue"
      
      b3 <- ggplot(dataframe, aes({{variable}})) +
              geom_density(fill = fill, colour = line) +
              #geom_vline(xintercept = mean_vble, size = 1, colour = "#FF3721",linetype = "dashed")+
             theme(plot.title = element_text(hjust = 0.5))
      
      #return(b3)
}
```


```{r}

grafico_categoricas <-function(dataframe,variable,titulo,eje_y) {

      #mean_vble  = round(mean(dataframe$variable, na.rm=T), digits = 1)
      
      fill <- "#4271AE"
      line <- "#1F3552"
      
      b3 <- ggplot(data = dataframe, aes(x = {{variable}})) + 
            geom_bar(color = 'darkslategray', fill = 'steelblue') + 
            xlab(eje_y) + 
            ylab("Cantidades") + 
            ggtitle(titulo) +
            coord_flip()+
            geom_text(stat='count',aes(label=..count..), vjust=-0.5, size=3) 
      
      return(b3)
}

#grafico_categoricas(df,loan_grade,s,ss)
```


```{r}

grafico_categoricas33 <-function(dataframe,variable) {

      b3 <- (ggplot(df, aes({{variable}}, fill={{variable}}))
           + geom_bar()

           + geom_text(
               aes(label=after_stat(..count..)),
               stat='count',
              nudge_x=-0.14,
               nudge_y=0.125,
              vjust=-0.5
           )
          + geom_text(
         aes(label=scales::percent(round((..prop..),2)), group=1),
         stat='count',
         nudge_x=0.14,
         nudge_y=0.125,
         va="middle",
         vjust=0.5,
         accuracy = 1L
         
         )
      )
      
      #return(b3)
}

```

```{r}

grafico_categoricas_biv <-function(dataframe,variable) {


b1=ggplot(dataframe) + 
        geom_density(aes(x = loan_amnt, fill = {{variable}}), position = 'stack') + 
        xlab("Importe") + 
        ylab("Frecuencia") + 
        ggtitle("Importe de prestamo") +
        theme_minimal()

      return(b1)
}
```


```{r}
grafico_categoricas_biv2 <-function(dataframe,variable) {
  
    p1 <- df %>% select(target, {{variable}}) %>%
      na.omit() %>%
      ggplot(aes(x={{variable}}, y=target, fill={{variable}})) +
      geom_boxplot()+
      theme(axis.text.x = element_blank(),
        axis.ticks.x = element_blank())
    
    return(p1)
}
```


```{r}
grafico_categoricas_biv3 <-function(dataframe,variable) {
  
    b1= ggplot(data = dataframe, aes(x = target, y = {{variable}})) + 
      geom_point(aes(color = {{variable}}), size = 1, alpha = 0.7) +
      geom_smooth(aes(color = {{variable}})) +
      facet_grid({{variable}} ~., scales = 'free') +
      xlab('Puntuación') + 
      ylab('Puntuación') +
      ggtitle('gggggggggg') + 
      theme_minimal()
    
      return(b1)
}
```


```{r}

test_normalidad <-function(df_var){
  
  anderson<-ad.test(df_var)
  cramer<-cvm.test(df_var)
  lillies<-lillie.test(df_var)

  ## Salida a pantalla 
 print(anderson)
 print(cramer)
 print(lillies)
 qqnorm(df_var)

}

```


## Análisis univariante


### Variable dependiente (target)

En primer lugar, se realiza un análisis de la variable dependiente (target).


```{r}
G1=grafico_numerico(train,target)
G1
```

```{r, warning=FALSE}
test_normalidad(train$target)
```


```{r }
des1=describe_v1(train,target)
    
  des1 %>%
  kbl() %>%
  kable_styling()
```


```{r }
des2=describe_v2(train,target)
    
  des2 %>%
  kbl() %>%
  kable_styling()

```


Se aprecia que no es una variable normal y tendremos que hacer una transformación.


### Variables independientes numéricas 


**Edad (años) (person_age)**	


```{r}
G2=grafico_numerico(train,person_age)
G2

```

```{r, warning=FALSE}
test_normalidad(train$person_age)
```

```{r }
des1=describe_v1(train,person_age)
    
  des1 %>%
  kbl() %>%
  kable_styling()

```


```{r }
des2=describe_v2(train,person_age)
    
  des2 %>%
  kbl() %>%
  kable_styling()
```


Se aprecia que no es una variable normal y tendremos que hacer una transformación.


**Ingresos personales (person_income)**


```{r}
G3=grafico_numerico(train,person_income)
G3

```

```{r }
des1=describe_v1(train,person_income)
    
  des1 %>%
  kbl() %>%
  kable_styling()

```


```{r }
des2=describe_v2(train,person_income)
    
  des2 %>%
  kbl() %>%
  kable_styling()
```


```{r, warning=FALSE}
test_normalidad(train$person_income)
```


```{r}
train %>% filter( person_income>1000000)
```

Se aprecia que no es una variable normal y tendremos que hacer una transformación. También que hay un registro con una ingreso descomunal y que tiene una edad errónea, se eliminará el registro.

**Tiempo trabajado (person_emp_length)**


```{r, warning=FALSE}
G4=grafico_numerico(train,person_emp_length)
G4

```

```{r, warning=FALSE}
# Test de normalidad
test_normalidad(train$person_emp_length)
```


```{r }

des1=describe_v1(train,person_emp_length)
    
  des1 %>%
  kbl() %>%
  kable_styling()

```


```{r }
des2=describe_v2(train,person_emp_length)
    
  des2 %>%
  kbl() %>%
  kable_styling()
```

Se aprecia que no es una variable normal y tendremos que hacer una transformación.

**Tasa de interés (loan_int_rate)**

```{r, warning=FALSE}
G5=grafico_numerico(train,loan_int_rate)
G5

```

```{r, warning=FALSE}
# Test de normalidad
test_normalidad(train$person_emp_length)
```

```{r }
des1=describe_v1(train, loan_int_rate)
    
  des1 %>%
  kbl() %>%
  kable_styling()

```


```{r }
des2=describe_v2(train,loan_int_rate)
    
  des2 %>%
  kbl() %>%
  kable_styling()
```


**Lapso de tiempo desde el anterior préstamo (cb_person_cred_hist_length)**


```{r}
G6=grafico_numerico(train,cb_person_cred_hist_length)
G6

```


```{r, warning=FALSE}
# Test de normalidad
test_normalidad(train$person_emp_length)
```


```{r }
des1=describe_v1(train,cb_person_cred_hist_length)
    
  des1 %>%
  kbl() %>%
  kable_styling()

```


```{r }
des2=describe_v2(train,cb_person_cred_hist_length)
    
  des2 %>%
  kbl() %>%
  kable_styling()
```


Esta variable está escalonada, por lo que se discretiza.

### Variables categóricas

**Relación con la propiedad de la vivienda (person_home_ownership)**


```{r, warning=FALSE}
a2=grafico_categoricas33(train,person_home_ownership)
a2
```


El valor infrecuente de OTHER se le asignará el valor más frecuente RENT.

**Motivo del préstamo (loan_intent)**


```{r, warning=FALSE}
a3=grafico_categoricas33(train,loan_intent)
a3
```


**Falta de pago de un préstamo antiguo (cb_person_default_on_file)**



```{r, warning=FALSE}
a4=grafico_categoricas33(train,cb_person_default_on_file)
a4
```



## Análisis bivariable


```{r}
grafico_categoricas_biv2(train, loan_intent)
```

```{r}
grafico_categoricas_biv2(train, person_home_ownership)
```


```{r}
grafico_categoricas_biv2(train,cb_person_default_on_file)
```




```{r, warning=FALSE}

#ggpairs(train, lower = list(continuous = "smooth"),
#        diag = list(continuous = "barDiag"), axisLabels = "none")
```


Sccater plot entre la variable dependiente, target y las independientes numéricas para ver relaciones


```{r, warning=FALSE}
bi1=ggplot(train, aes(x=person_age, y=target)) +
  geom_point() +
  stat_smooth(method=lm)

bi2=ggplot(train, aes(x=person_income, y=target)) +
  geom_point() +
  stat_smooth(method=lm)

bi3=ggplot(train, aes(x=person_emp_length, y=target)) +
  geom_point() +
  stat_smooth(method=lm)

bi4=ggplot(train, aes(x=loan_int_rate, y=target)) +
  geom_point() +
  stat_smooth(method=lm)

plot_grid(bi1, bi2,bi3,bi4)
```



A coninutación, se procede a realiazar la depuración que hemos visto en el EDA:

Eliminación del registro erróneo, que tiene unos ingresos por persona de 6.000.000 y 144 años

```{r}
dim(train)
train <- train %>% filter(person_income< 6000000)
dim(train)
```


Para las variables numéricas imputaremos los valores superiores al percentil 99 con dicho percentil:


```{r}
#Mejora....con una función si da tiempo o incluso dentro de un mismo train <- train %>%
train <- train %>%
  mutate(target= ifelse(target>=quantile(train$target,0.99),quantile(train$target,0.99), target))

train <- train %>%
  mutate(person_income= ifelse(person_income>=quantile(train$person_income,0.99, na.rm = T), quantile(train$person_income,0.99, na.rm = T), person_income))

train <- train %>%
  mutate(person_emp_length= ifelse(person_emp_length>=quantile(train$person_emp_length,0.99, na.rm = T) ,quantile(train$person_emp_length,0.99, na.rm = T), person_emp_length))

train <- train %>%
  mutate(loan_int_rate= ifelse(loan_int_rate>quantile(train$loan_int_rate,0.99, na.rm = T), quantile(train$loan_int_rate,0.99, na.rm = T), loan_int_rate))

train <- train %>%
  mutate(loan_int_rate= ifelse(loan_int_rate>quantile(train$loan_int_rate,0.99, na.rm = T), quantile(train$loan_int_rate,0.99, na.rm = T), loan_int_rate))

train <- train %>%
  mutate(person_age= ifelse(person_age>quantile(train$person_age,0.99, na.rm = T), quantile(train$person_age,0.99, na.rm = T), person_age))

```



Para la variable categórica relación con la casa (person_home_ownership) la categoría OTHER tiene muy pocos casos y le asignamos el valor más frecuente, en este caso RENT.


```{r}
table(train$person_home_ownership)
train$person_home_ownership[train$person_home_ownership=="OTHER"] <-"RENT"
table(train$person_home_ownership)

```
Para la variable lapso de tiempo desde el anterior préstamo parece qe tiene escalones por lo que decidimos discretizar variable.
 

```{r}
train$cb_person_cred_hist_length_cat <- cut(train$cb_person_cred_hist_length,c(0,4,10,Inf),
                                            labels = c("0-4", "5-10", "Más de 10") )

#table(train$cb_person_cred_hist_length_cat)
str(train$cb_person_cred_hist_length_cat)

kable(train %>% group_by(cb_person_cred_hist_length_cat) %>% 
        summarise(`Frecuencia relativa` = n()/nrow(train)*100,`d` = n()))
```



## Detección e imputación de datos faltantes


En el EDA ya hemos visto los missing que tenemos para cada una de nuestras variables, ahora haremos un gráfico para ver recopilar que variables tienen missing y si hay alguna relación entre los registro que tienen missing, por si se concentran en casos concretos.
En clase se planteo hacer este paso antes, pero no sé hizo de esa forma porque quería hacer las imputaciones de las variables con los percentiles 99 sin tener en cuenta los valores imputados de los missing


```{r}

aggr_plot <- aggr(train, col=c('navyblue','red'), numbers=TRUE, sortVars=TRUE,
                  labels=names(train), cex.axis=.4, gap=3, 
                  ylab=c("Histogram of missing data","Pattern"))


```

Observamos que no están concentrados en casos, y sólo tenemos missing en dos variables. Para ver el potencial de esta visión de estudio, hacemos un gráfico de las dos variables que tienen missing para ver si encontramos algún patrón:


```{r}
train %>% select(loan_int_rate, person_emp_length) %>% marginplot()
```

```{r}

train %>% select(loan_int_rate, target) %>% marginplot()

```

Generamos un dataframe con aquellos casos que tienen valores missing en cualquiera de los dos campos por si se quisiera ver en su conjunto la tabla, pero donde se ven las relaciones es en los gráficos anteriores.

```{r}
df_train_missing <-train %>% 
        filter(is.na(person_emp_length) | is.na(loan_int_rate))
```



Como solución parcial imputaremos el valor de la mediana siendo conscientes que no es la mejor solución.

```{r}

train$person_emp_length[is.na(train$person_emp_length)] <-median(train$person_emp_length, na.rm=T)
train$loan_int_rate[is.na(train$loan_int_rate)] <-median(train$loan_int_rate, na.rm=T)

#train %>%   filter(loan_int_rate == NA & person_emp_length == NA)

#train %>%   filter(loan_int_rate == NA & person_emp_length == NA)
```

Comprobación rápida que ya no tenemos missing
```{r}
apply(is.na(train),2,FUN="mean")*100

```

https://cran.r-project.org/web/packages/simputation/vignettes/intro.html

```{r}



```


## Transformaciones de variables cuantitativas


**Importe del prestamo (loan_amnt/Target)**

Utilizamos la librería bestNormalize que nos da la opción de sacar multiples transformaciones y te indica cuál es la mejor.

```{r}


    (arcsinh_obj_tar <- arcsinh_x(train$target))
    (boxcox_obj_tar <- boxcox(train$target))
    (yeojohnson_obj_tar <- yeojohnson(train$target))
    (orderNorm_obj_tar <- orderNorm(train$target))
     (BNobject_tar <- bestNormalize(train$target)) 
  

    par(mfrow = c(2,2))
    MASS::truehist(arcsinh_obj_tar$x.t, main = "Arcsinh transformation", nbins = 12)
    MASS::truehist(boxcox_obj_tar$x, main = "Box Cox transformation", nbins = 12)
    MASS::truehist(yeojohnson_obj_tar$x, main = "Yeo-Johnson transformation", nbins = 12)
    MASS::truehist(orderNorm_obj_tar$x, main = "orderNorm transformation", nbins = 12)


```


En este caso la mejor es orderNorm (The Ordered Quantile technique), aunque la de Box Cox que era la que se barajaba en un principio es muy similar. 

Lunción orderNorm hace una normalización de cuantil ordenado (ORQ) que utiliza un mapeo de rango de los datos observados a la distribución normal para garantizar una distribución transformada normal (si los lazos no están presentes). La normalización ORQ funciona de manera muy consistente
a través de diferentes distribuciones, normalizando con éxito datos sesgados a la izquierda o a la derecha, datos multimodales.


```{r}
train$target_tr <- orderNorm_obj_tar$x
```


**Importe del prestamo (person_age)**


```{r}

    (arcsinh_obj_age <- arcsinh_x(train$person_age))
    (boxcox_obj_age <- boxcox(train$person_age))
    (yeojohnson_obj_age <- yeojohnson(train$person_age))
    (orderNorm_obj_age <- orderNorm(train$person_age))
     (BNobject_age <- bestNormalize(train$person_age)) 
  

    
    par(mfrow = c(2,2))
    MASS::truehist(arcsinh_obj_age$x.t, main = "Arcsinh transformation", nbins = 12)
    MASS::truehist(boxcox_obj_age$x, main = "Box Cox transformation", nbins = 12)
    MASS::truehist(yeojohnson_obj_age$x, main = "Yeo-Johnson transformation", nbins = 12)
    MASS::truehist(orderNorm_obj_age$x, main = "orderNorm transformation", nbins = 12)
```

En este caso la mejor es orderNorm (The Ordered Quantile technique).

```{r}
train$person_age_tr <- orderNorm_obj_age$x
```


**Edad (person_income)**


```{r}


    (arcsinh_obj_in <- arcsinh_x(train$person_income))
    (boxcox_obj_in <- boxcox(train$person_income))
    (yeojohnson_obj_in <- yeojohnson(train$person_income))
    (orderNorm_obj_in <- orderNorm(train$person_income))
     (BNobject_in <- bestNormalize(train$person_income)) 
  

    par(mfrow = c(2,2))
    MASS::truehist(arcsinh_obj_in$x.t, main = "Arcsinh transformation", nbins = 12)
    MASS::truehist(boxcox_obj_in$x, main = "Box Cox transformation", nbins = 12)
    MASS::truehist(yeojohnson_obj_in$x, main = "Yeo-Johnson transformation", nbins = 12)
    MASS::truehist(orderNorm_obj_in$x, main = "orderNorm transformation", nbins = 12)

```

En este caso la mejor es orderNorm (The Ordered Quantile technique).

```{r}
train$person_income_tr <- orderNorm_obj_in$x
```


**Tiempo trabajado (person_emp_length)**



```{r}



    (arcsinh_obj_len <- arcsinh_x(train$person_emp_length))

    (yeojohnson_obj_len <- yeojohnson(train$person_emp_length))
    (orderNorm_obj_len <- orderNorm(train$person_emp_length))
    #(boxcox_obj <- boxcox(train$person_emp_length))
    (BNobject_len <- bestNormalize(train$person_emp_length)) 
    (sqrt_x_object_len <- sqrt_x(train$person_emp_length))

    
    par(mfrow = c(2,2))
    MASS::truehist(arcsinh_obj_len$x.t, main = "Arcsinh transformation", nbins = 12)
    #MASS::truehist(boxcox_obj$x, main = "Box Cox transformation", nbins = 12)
    MASS::truehist(yeojohnson_obj_len$x, main = "Yeo-Johnson transformation", nbins = 12)
    MASS::truehist(orderNorm_obj_len$x, main = "orderNorm transformation", nbins = 12)
    MASS::truehist(sqrt_x_object_len$x, main = "SQRT transformation", nbins = 12)



    #str(train$person_emp_length)
```


En este caso la mejor es la raiz cuadrada.

```{r}
train$person_emp_length_tr <- sqrt_x_object_len$x

```







```{r, warning=FALSE}
bi1=ggplot(train, aes(x=person_age, y=target)) +
  geom_point() +
  stat_smooth(method=lm)

bi2=ggplot(train, aes(x=person_income, y=target)) +
  geom_point() +
  stat_smooth(method=lm)

bi3=ggplot(train, aes(x=person_emp_length, y=target)) +
  geom_point() +
  stat_smooth(method=lm)

bi4=ggplot(train, aes(x=loan_int_rate, y=target)) +
  geom_point() +
  stat_smooth(method=lm)

plot_grid(bi1, bi2,bi3,bi4)

```



## Correlaciones

Calculamos las correlaciones para las variables numéricas. Este paso se podría haber hecho también en la fase de EDA, de hecho es recomendable, pero cuando hay datos de missing se recomienda tmabién después dada la sensibilidad de estos calculos cuando existen missing.

Las correlaciones más habituales son las de Pearson, Kendall y Sperman.

```{r}
corr_matrix <- train %>% select_if(is.numeric) %>% 
  select(-c(loan_status,loan_percent_income)) %>%
  cor(method="pearson", use="pairwise.complete.obs")


```


```{r,warning=FALSE}
#square
corrplot(corr_matrix, method = "square", shade.col = NA, tl.col = "black",
         tl.srt = 35,  addCoef.col = "black", addcolorlabel = "no",
         order = "hclust", type = "upper", diag = F, addshade = "all")
```

Se obserava las correlaciones de valor 1 de las variables transformadas con las de su origen, también destaca la edad y el lapso de tiempo desde que pediste el último crédito, lógica y además esta última variable la hemos discretizado por lo que no se usará la numérica y por otro lado la más interesante nuestra variable target, importe de préstamo, con los ingresos de la persona que le dán el crédito.


# Regresión

## Regresión simple

Primero haremos una regresión simple con la variable de mayor correlación con la variable dependiente (target ~  ingresos):

```{r}
lm_fit <- lm(target_tr~person_income_tr, data=train)
summary(lm_fit)

```


El p-valor del modelo es significativamente bajo, 2.2e-16, por lo que se puede aceptar que el modelo no es por azar. Por los parámetros, tanto el de la constante como el de ingresos tienen p-valores muy bajos también por lo que podemos rechazar la hipótesis nula. En cuanto al R cuadrado explica el 17,51% de la variabilidad de y.

Y el Residual standard error es de 5685., el coeficiente constante es 5.102e+03 y el de person_income_tr, 6.954e-02

La interpretación de los coeficientes al ser muy bajos es dificil de interpretar, por lo que pondré otro modelo para explicar:

importe_prestamo= 47,60 + 3,01 × ingresos

En este ejemplo los coeficientes de regresión son 47,60, para la constante y 3,01 para ingresos y la interpretación es cuando aumentamos en una unidad en los ingresos, los prestamos aumentan en 3,01 unidades.

## Regresión multiple


### Métodos de selección de variables


Para este apartado nos basaremos fielmente en el trabajo elaborado por Victor Aceña en noviembre de 2021.

Usaremos el método Subset que consiste en generar todas las combinaciones de variables posibles y quedarse con el “mejor” subconjunto de ellas. Considerando mejor, el que obtenga una puntuación superior para alguna métrica de evaluación definida previamente.

Como se observa a continuación, para implementar este método se utilizar la función regsubsets de la librería leaps.

```{r, warning=FALSE}
regfit_full <- leaps::regsubsets(target_tr~ person_age_tr+ person_income_tr+
                                   person_emp_length_tr+loan_int_rate+loan_intent
                                 +cb_person_cred_hist_length_cat+ person_home_ownership, 
                                 data=train,nvmax=5)
```


Mediante la función summary, podemos ver qué variables están presentes en cada uno de los subconjuntos. Además, se observa que por el número de subconjuntos evaluados está limitado a cinco, por defecto son ocho. Através de los asteriscos vemos que una variable está incluida en el correspondiente modelo.


```{r}
reg_sum <- summary(regfit_full)
reg_sum
```


Para poder evaluar los modelos, podemos consultar los vectores de métricas almacenados mediante summary, por ejemplo el R cuadrado ajustado, el criterio de información de Akaike,Cp (AIC y criterio de información bayesiano, BIC .



```{r}
reg_sum$adjr2
reg_sum$bic
```


Para evaluarlo de una manera visual, el paquete tiene una función plot incorporada que nos indica la mejor combinación probada en función de las distintas métricas disponibles.

```{r}
for (metric in c("r2", "adjr2", "Cp", "bic")){plot(regfit_full, scale=metric)}

```

Una vez se haya seleccionado uno de los modelos, se pueden obtener los coeficientes para cada una de las variables con la instrucción coef.

En este caso se seleccionaría el modelo primero es el que tendría el BIC más pequeño, aunque el R cuadrado ajustado no sea el mas alto, pero siempre tiene que primar que el modelo cuanto más simple mejor, principio de parsimonia. Recordamos que el R cuadrado (sin ajustar) no se tiene que ver para la regresión multiple, recordemos que este R, aumentará siempre y cuando metas más variables, por eso se usa el ajustado que penaliza el incluir variables. 

```{r}
coef(regfit_full, 1)
```

Existen otros métodos como:

Forward Stepwise: este método parte de un conjunto de variables vacío, de manera que este se incrementa con la variable que más mejora el modelo para la métrica seleccionada. Se incluye una variable a cada paso, hasta que se llega al número seleccionado o el modelo no mejora más.

Backward Stepwise: al contaratio que el forward se parte del conjunto de todas las variables, de manera que este se disminuye con la variable que más mejora el modelo para la métrica seleccionada. Se elimina una variable a cada paso, hasta que se llega al número seleccionado o el modelo no mejora más

Y luego existe el método clasico de ir partiendo de todas las variables e ir sacando y metiendo en función de los contrastes de hipótesis de los parámetros y del conocimiento de la materia a estuadiar.


Siempre es aconsejable barajar varios metodos y contrastarlos y hacer muchas pruebas, hoy en día es más sencillo dado que las máquinas pueden soportarlo.


Gráficos por número de variables y R ajutado

```{r}
par ( mfrow =c(2 ,3) )
plot(reg_sum$rss , xlab ="Number of Variables", ylab ="RSS ",type ="l")
plot(reg_sum$adjr2 ,xlab ="Number of Variables",ylab ="Adjusted RSq", type ="l")
plot(reg_sum$bic ,xlab ="Number of Variables",ylab ="BIC", type ="l")

```


Regresión con las variables seleccionadas:

```{r}
summary(modelo_final<-lm(target_tr~person_income_tr, train))
```

Se vé que los contrastes de los parámetros de la constante y de los ingresos transformados se rechaza la hipótesis nula de que son nulos.
Tiene un R cuadradro ajustado de 0.1751, es decir explica un 17,51% de la variación de la variable dependiente.

#### Diagnóstico del modelo


Realizamos el análisis de residuos del modelo final:



Por test de normalidad y por gráfico vemos que los residuos son normales

```{r}
# Test de hipótesis para el análisis de normalidad de los residuos
test_normalidad(modelo_final$residuals)
```

Una visión más amplia de los residuos:

```{r}
par(mfrow=c(2,2))
plot(modelo_final)
```

Los residuos son de la comparacion de residuos y las estimaciones no tienen una forma de embudo que evidencian la homocedasticidad, pero no se ve una clara aleatoriedad.



Test de contraste de homocedasticidad Breusch-Pagan

Hipótesis nula: los errores tienen varianza constante.
H1:los errores no tienen varianza constante.

```{r}
bptest(modelo_final)
```

Como el p-valor es pequeño entonces hay evidencias para rechazar la hipótesis nula de homocedasticidad.



```{r}
# Detección y visualización de observaciones influyentes

influencePlot(modelo_final)

```
¿Hay colinealidad?

Al ser un modelo tan simple, no puede, pero para evaluarlo se calculan los factores de inlación de la varianza (VIF)

Cómo interpretar los valores de VIF:
*El valor de VIF comienza en 1 y no tiene límite superior. Una regla general para interpretar los VIF es la siguiente:

*Un valor de 1 indica que no hay correlación entre una variable explicativa dada y cualquier otra variable explicativa en el modelo.
*Un valor entre 1 y 5 indica una correlación moderada entre una variable explicativa dada y otras variables explicativas en el modelo, pero esto a menudo no es lo suficientemente grave como para requerir atención.
*Un valor mayor que 5 indica una correlación potencialmente severa entre una variable explicativa dada y otras variables explicativas en el modelo. En este caso, las estimaciones de los coeficientes y los valores p en el resultado de la regresión probablemente no sean confiables.

Una vez obtenido el valor de VIF para cada una de las variables independientes de un conjunto de datos es posible identificar las variables más dependientes y eliminarlas. El proceso que se debería seguir para solucionar la multicolinealidad con VIF es:

1.Obtener el VIF para todas las variables independientes
2.Identificar la que tiene el valor máximo de VIF, solamente una, aunque existan dos o más con el mismo valor
3.Si esta variable supera un valor umbral, por ejemplo 5, eliminarla y volver al punto 1. En caso contrario se termina el proceso.

Es importante eliminar únicamente una variable en cada paso, ya que en caso contrario se podría eliminar todas las variables relacionadas. Por ejemplo, si tenemos una variable que es dos veces otra, en tal caso ambas tendrán un valor de VIF que tiende a infinito, ya que el R^2 es igual a uno. Si eliminamos ambas se eliminan todas las ocurrencias de esa variable, que no es lo que se desea.


```{r}
# Factores de inflación de la varianza......DARIA ERROR AL SOLO TENER UN TERMINO, UNA VARIALE INDEPENDIENTE
#car::vif(modelo_final)
```


### Validación


Realizaremos las mismas modificacines al fichero de test que hicimos al de train para poder ejecutar el modelo en las mismas condiciones.

```{r}

validacion$person_home_ownership[validacion$person_home_ownership=="OTHER"] <-"RENT"


validacion$cb_person_cred_hist_length_cat <- cut(validacion$cb_person_cred_hist_length,c(0,4,10,Inf),
                                            labels = c("0-4", "5-10", "Más de 10") )

validacion$person_emp_length[is.na(validacion$person_emp_length)] <-median(train$person_emp_length, na.rm=T)
validacion$loan_int_rate[is.na(validacion$loan_int_rate)] <-median(train$loan_int_rate, na.rm=T)



```



Comprobación rápida que ya no tenemos missing

```{r}
apply(is.na(validacion),2,FUN="mean")*100

```


Transformaciones de variables para que sigan la hipótesis de normalidad de los datos.


```{r}

(orderNorm_obj_age_val <- orderNorm(validacion$person_age))
train$person_age_tr <- orderNorm_obj_age_val$x

(orderNorm_obj_in_val <- orderNorm(validacion$person_income))
train$person_income_tr <- orderNorm_obj_in_val$x

(sqrt_x_object_len_val <- sqrt_x(validacion$person_emp_length))
train$person_emp_length_tr <- sqrt_x_object_len_val$x
```




Comparamos que los datos de train y de validacion siguen las misma distribución por variable, esta fase es muy importante porque cualquier cambio de distribución en alguna variable que luego entra en el modelo puede provocar que no ajuste bien el modelo, en este caso al ser una muestra aleatoria del mismo dataframe es muy complicado que pase, pero si el fichero de validación es de otro periodo estacional, podría, por lo que esta fase es siempre esencial.

Aunque a priori sólo se podría ver en aquellas variables que entran en el modelo, es recomendable ver también las más relvantes de tus datos por conocimientos de la materia por si ya se aprecian otros cambios y se pueden encontrar explicaciones de los mismos.


Lo ideal es poner gráficos superpuestos....para ver mejor los cambios e incluir información de valores, missing, media, mediana, percentiles, porcentajes de 0....te puede pasar que en los nuevos datos, validación, tengas un missing y en los de entrenamiento, no,... (lo normal es que te dé error al ejecutar el modelo, pero si en la variable de entrenamiento no tiene ceros y luego en el de validación sí, que están enmascarados los valores missing, el modelo no te daría error, pero los resultados serán una sorpresa)


**Edad (años) (person_age)**	


```{r}
G2_val=grafico_numerico(validacion,person_age)
G2

```



**Ingresos personales (person_income)**


```{r}
G3_val=grafico_numerico(validacion,person_income)
G3

```



**Tiempo trabajado (person_emp_length)**


```{r, warning=FALSE}
G4_val=grafico_numerico(validacion,person_emp_length)
G4

```



**Tasa de interés (loan_int_rate)**

```{r, warning=FALSE}
G5_val=grafico_numerico(validacion,loan_int_rate)
G5

```


**Lapso de tiempo desde el anterior préstamo (cb_person_cred_hist_length)**


```{r}
G6_val=grafico_numerico(validacion,cb_person_cred_hist_length)
G6

```



**Relación con la propiedad de la vivienda (person_home_ownership)**


```{r, warning=FALSE}
a2_val=grafico_categoricas33(validacion,person_home_ownership)
a2
```


El valor infrecuente de OTHER se le asignará el valor más frecuente RENT.

**Motivo del préstamo (loan_intent)**


```{r, warning=FALSE}
a3_val=grafico_categoricas33(validacion,loan_intent)
a3
```


**Falta de pago de un préstamo antiguo (cb_person_default_on_file)**



```{r, warning=FALSE}
a4_val=grafico_categoricas33(validacion,cb_person_default_on_file)
a4
```


















https://rpubs.com/dsulmont/740953

https://rpubs.com/Jo_/regresion_lineal_simple

## Evaluación de modelos

https://rpubs.com/Joaquin_AR/226291

https://rpubs.com/Cristina_Gil/Regresion_Lineal_Simple


https://rpubs.com/Cristina_Gil/Regresion_Lineal_Multiple

5. Validación de condiciones

• plot(modelo) -> Análisis de los residuos (distribución, variabilidad…)

• shapiro.test(modelo$residuals) -> Test de hipótesis de Shapiro Wilk para el análisis de normalidad

• plot(predict(modelo), rstudent(modelo)) -> Residuos estudentizados para detección de outliers o puntos influyentes

• bptest(modelo) -> Test de contraste de homocedasticidad Breusch-Pagan

• influence.measures(modelo) -> Detección de observaciones influyentes

• influencePlot(modelo) -> Detección y visualización de observaciones influyentes

• outlierTest(modelo) -> Test de detección de outliers

• vif(modelo) -> Calcula VIFs (factor de inflación de la varianza)



# Versiones de los paquetes de R

```{r}
 sesion_info <- devtools::session_info()
dplyr::select(
  tibble::as_tibble(sesion_info$packages),
  c(package, loadedversion, source)
)

```



